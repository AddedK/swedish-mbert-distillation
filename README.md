# swedish-mbert-distillation
The code and documentation for my Master's Thesis, which is about applying task-agnostic knowledge distillation on Swedish pre-trained mBERT models.

A link to the written report will be added in the future.

The azureML directory contains the files that were used together with Microsoft Azure Machine Learning, which includes the code for pre-training and task-agnostic distillation.

The SUCX_ft and wikiannEnFT notebooks are used to fine-tune the pre-trained teacher or student models on the respective datasets.

The code for evaluating the student models on the OverLim dataset can be found here: https://github.com/kb-labb/overlim_eval
